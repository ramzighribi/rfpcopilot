'use server';

import OpenAI, { AzureOpenAI } from 'openai';
import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
import { LLMConfig } from "@/store/useProjectStore";

// Fonction helper pour cr√©er un client Azure OpenAI avec support Entra ID
function createAzureOpenAIClient(config: LLMConfig) {
  const { endpoint, apiKey, apiVersion, deployment } = config;
  
  // Si pas de cl√© API, utiliser l'authentification Entra ID
  if (!apiKey || apiKey === '') {
    const credential = new DefaultAzureCredential();
    const azureADTokenProvider = getBearerTokenProvider(
      credential,
      "https://cognitiveservices.azure.com/.default"
    );
    return new AzureOpenAI({ 
      endpoint, 
      apiVersion, 
      deployment,
      azureADTokenProvider
    });
  }
  
  // Sinon, utiliser l'authentification par cl√© API
  return new AzureOpenAI({ endpoint, apiKey, apiVersion, deployment });
}

// ## FONCTION 1 : TEST DE CONNEXION SIMPLE ##
export async function testLLMConnection(config: LLMConfig) {
  if (config.provider !== 'Azure OpenAI') {
    if (config.apiKey.length > 5) return { success: true, message: `Connexion simul√©e pour ${config.provider} r√©ussie.`};
    return { success: false, message: "Fournisseur non support√© pour un test r√©el."}
  }
  const { endpoint, apiVersion, deployment } = config;
  if (!endpoint || !apiVersion || !deployment) return { success: false, message: "Endpoint, API version et deployment sont requis pour Azure." };
  
  try {
    const client = createAzureOpenAIClient(config);
    await client.chat.completions.create({
      model: deployment,
      messages: [{ role: "system", content: "Test connection." }],
      max_completion_tokens: 5,
    });
    return { success: true, message: "Connexion √† Azure OpenAI r√©ussie !" };
  } catch (error: unknown) {
    const errorMessage = error instanceof Error ? error.message : 'Une erreur inconnue est survenue';
    
    // Message d'erreur plus d√©taill√© pour le 403
    if (errorMessage.includes('403') || errorMessage.includes('authentication')) {
      return { 
        success: false, 
        message: `Erreur d'authentification : ${errorMessage}. V√©rifiez que l'authentification par cl√© API est activ√©e ou utilisez Entra ID.` 
      };
    }
    
    return { success: false, message: `La connexion a √©chou√© : ${errorMessage}` };
  }
}

// ## FONCTION 2 : D√âBOGAGE DE CONNEXION AVANC√â ##
export async function debugAzureConnection(config: LLMConfig): Promise<{ success: boolean; logs: string[] }> {
  'use server';
  const logs: string[] = [];
  const { deployment } = config;
  const log = (message: string) => logs.push(`[${new Date().toLocaleTimeString('fr-FR')}] ${message}`);
  log("D√©but du test de d√©bogage...");
  try {
    log("Initialisation du client AzureOpenAI...");
    const client = createAzureOpenAIClient(config);
    log("Client initialis√© avec succ√®s.");
    log("Envoi d'un message de test...");
    const response = await client.chat.completions.create({
      model: deployment,
      messages: [{ role: "user", content: "Quelle est la capitale de la France ?" }],
      max_completion_tokens: 50,
    });
    log("R√©ponse re√ßue de l'API !");
    if (response.choices && response.choices.length > 0) {
      const choice = response.choices[0];
      const finishReason = choice.finish_reason;
      log(`Raison de la fin (finish_reason): ${finishReason}`);
      if (finishReason === 'content_filter') {
        log("üî¥ DIAGNOSTIC: Le filtre de contenu d'Azure a bloqu√© la r√©ponse.");
        return { success: false, logs };
      }
      if (choice.message?.content) {
        logs.push("--- D√âBUT DE LA R√âPONSE ---", choice.message.content, "--- FIN DE LA R√âPONSE ---");
        log("‚úÖ TEST R√âUSSI !");
        return { success: true, logs };
      }
    }
    log("‚ö†Ô∏è AVERTISSEMENT: La structure de la r√©ponse est inattendue.");
    return { success: false, logs };
  } catch (error: unknown) {
    log("‚ùå ERREUR CRITIQUE PENDANT L'APPEL API.");
    const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue';
    log(`Message: ${errorMessage}`);
    return { success: false, logs };
  }
}

// ## FONCTION 3 : G√âN√âRATION DE R√âPONSE UNIQUE (AVEC LOGIQUE POUR ANTHROPIC) ##
export async function generateSingleLLMResponse(
  question: string,
  llmConfigs: LLMConfig[],
  generationParams: Record<string, string>
): Promise<{ result: Record<string, string>, logs: string[] }> {
  'use server';

  const logs: string[] = [];
  const log = (message: string) => logs.push(`[${new Date().toLocaleTimeString('fr-FR')}] ${message}`);
  const startTime = performance.now();

  if (!question) throw new Error("La question est vide.");
  const validatedConfigs = llmConfigs.filter(c => c.isValidated);
  if (validatedConfigs.length === 0) throw new Error("Aucun LLM n'a √©t√© valid√©.");

  const rowResult: Record<string, string> = { question: question, status: 'Refus√©e' };

  let lengthInstruction = '';
  switch (generationParams.responseLength) {
    case 'Courte': lengthInstruction = 'Ta r√©ponse doit √™tre tr√®s courte, concise, et ne faire que 1 √† 2 lignes au maximum.'; break;
    case 'Moyenne': lengthInstruction = 'Ta r√©ponse doit √™tre de taille moyenne, entre 3 et 5 lignes.'; break;
    case 'Longue': lengthInstruction = 'Ta r√©ponse doit √™tre longue, d√©taill√©e, et faire plus de 5 lignes.'; break;
  }
  const languageInstruction = generationParams.language === 'Fran√ßais' ? 'R√©ponds imp√©rativement et exclusivement en Fran√ßais.' : 'Answer imperatively and exclusively in English.';
  const systemPrompt = `Agissez en tant que : ${generationParams.persona}. ${languageInstruction} ${lengthInstruction} ${generationParams.instructions}`;
  
  for (const config of validatedConfigs) {
    const providerStartTime = performance.now();
    log(`  [${config.provider}] D√©but du traitement.`);
    try {
      if (config.provider === 'Google') {
        const GOOGLE_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/${config.model}:generateContent?key=${config.apiKey}`;
        const requestBody = { contents: [{ parts: [{ text: systemPrompt + "\n\n" + question }] }] };
        
        log(`  [${config.provider}] Envoi de la requ√™te √† l'API...`);
        const apiStartTime = performance.now();
        const response = await fetch(GOOGLE_API_URL, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(requestBody) });
        const apiEndTime = performance.now();
        log(`  [${config.provider}] R√©ponse re√ßue en ${Math.round(apiEndTime - apiStartTime)}ms.`);

        if (!response.ok) {
          const errorBody = await response.json();
          throw new Error(`Erreur API Google: ${response.status} - ${errorBody.error.message}`);
        }
        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text;
        if (data.promptFeedback?.blockReason) {
          rowResult[config.provider] = `ERREUR: Bloqu√© par le filtre de contenu. Raison: ${data.promptFeedback.blockReason}`;
        } else if (text) {
          rowResult[config.provider] = text;
          rowResult.status = 'Valid√©e';
        } else {
          rowResult[config.provider] = "ERREUR: La r√©ponse de Google √©tait vide ou mal form√©e.";
        }
      } else {
        let client: OpenAI | AzureOpenAI;
        let modelName: string;
        const completionPayload: Record<string, unknown> = { messages: [{ role: "system", content: systemPrompt }, { role: "user", content: question }] };

        switch(config.provider) {
          case 'Azure OpenAI': 
            client = createAzureOpenAIClient(config); 
            modelName = config.deployment; 
            completionPayload.max_completion_tokens = 4096; 
            break;
          case 'OpenAI': client = new OpenAI({ apiKey: config.apiKey, timeout: 60 * 1000 }); modelName = 'gpt-4o'; completionPayload.max_tokens = 4096; break;
          case 'Anthropic':
            client = new OpenAI({
              apiKey: config.apiKey,
              baseURL: 'https://api.anthropic.com/v1',
              timeout: 60 * 1000,
              defaultHeaders: { 'x-anthropic-version': '2023-06-01', 'max-tokens': '4096' }
            });
            modelName = 'claude-3-opus-20240229';
            // Pour Anthropic, le system prompt est un param√®tre de premier niveau
            completionPayload.system = systemPrompt;
            completionPayload.messages = [{ role: "user", content: question }]; // On ne garde que le message user
            break;
          case 'Mistral': client = new OpenAI({ apiKey: config.apiKey, baseURL: 'https://api.mistral.ai/v1', timeout: 60 * 1000 }); modelName = 'mistral-large-latest'; completionPayload.max_tokens = 4096; break;
          default: rowResult[config.provider] = `[Fournisseur non impl√©ment√©]`; continue;
        }
        
        completionPayload.model = modelName;
        log(`  [${config.provider}] Envoi de la requ√™te √† l'API...`);
        const apiStartTime = performance.now();
        const response = await client.chat.completions.create(completionPayload as any);
        const apiEndTime = performance.now();
        log(`  [${config.provider}] R√©ponse re√ßue en ${Math.round(apiEndTime - apiStartTime)}ms.`);
        
        const choice = response.choices[0];
        if (choice && choice.message?.content) {
          rowResult[config.provider] = choice.message.content;
          rowResult.status = 'Valid√©e';
        } else {
          rowResult[config.provider] = `ERREUR: R√©ponse vide. Raison: ${choice?.finish_reason || 'inconnue'}.`;
        }
      }
    } catch (error: unknown) {
      const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue';
      rowResult[config.provider] = `ERREUR: ${errorMessage}`;
    }
    const providerEndTime = performance.now();
    log(`  [${config.provider}] Traitement termin√© en ${Math.round(providerEndTime - providerStartTime)}ms.`);
  }
  
  const totalEndTime = performance.now();
  log(`Traitement total de la question termin√© en ${Math.round(totalEndTime - startTime)}ms.`);
  
  return { result: rowResult, logs };
}

